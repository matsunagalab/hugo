---
draft: false
date: 2020-09-20T18:30:26+09:00
description: Lab-only
sidebar: right
categories:
  - "Lab"
---

### PCクラスタ設定メモ

---

### Slurm

#### mungeのインストール

ubuntuの場合はパッケージでインストールできる。
パッケージをインストールすると `munge` というユーザが自動的に作られる。

```bash
sudo apt update
sudo apt install -y munge libmunge-dev libmunge2
```

鍵ファイル `munge.key` を作成して共有。鍵は以下に置く。
```bash
/etc/munge/munge.key
```
`munge.key` のパーミッションは `0600` でなければならない。

#### slurmのインストール

コンパイルしてインストール。defaultでは `/usr/local` 以下へインストールされる。
Contorlのための `slumctld` と clientのための `slurmd` デーモンが作成される(他のもあるが割愛)。
```bash
./configure --with-hdf5=no
make
sudo make install
```

`slurm.conf` を作成して以下へ置く。全てのノードで同じ内容のファイルを置く。
```
/usr/local/etc/slurm.conf
```

`slurm.conf` の中身は以下。コア数などは `lscpu` や `sudo slurmd -C` などで確認しながら記入する。

```bash
# slurm.conf file generated by configurator easy.html.
# Put this file on all nodes of your cluster.
# See the slurm.conf man page for more information.
#
SlurmctldHost=crab
# 
#MailProg=/bin/mail 
MpiDefault=none
#MpiParams=ports=#-# 
ProctrackType=proctrack/pgid
ReturnToService=1
SlurmctldPidFile=/var/run/slurmctld.pid
#SlurmctldPort=6817 
SlurmdPidFile=/var/run/slurmd.pid
#SlurmdPort=6818 
SlurmdSpoolDir=/var/spool/slurmd
#SlurmUser=slurm
SlurmdUser=root 
StateSaveLocation=/var/spool
SwitchType=switch/none
TaskPlugin=task/affinity
# 
# 
# TIMERS 
#KillWait=30 
#MinJobAge=300 
#SlurmctldTimeout=120 
#SlurmdTimeout=300 
# 
# 
# SCHEDULING 
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
# 
# 
# LOGGING AND ACCOUNTING 
AccountingStorageType=accounting_storage/none
ClusterName=cluster
#JobAcctGatherFrequency=30 
JobAcctGatherType=jobacct_gather/none
#SlurmctldDebug=info 
#SlurmctldLogFile=
#SlurmdDebug=info 
#SlurmdLogFile=
# 
# 
# COMPUTE NODES 
GresTypes=gpu

NodeName=crab CPUs=64 RealMemory=512000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN
NodeName=n[1-2] Gres=gpu:titan:10 CPUs=64 RealMemory=64000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN
NodeName=n[3-4] Gres=gpu:1080:10 CPUs=64 RealMemory=64000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=2 State=UNKNOWN
NodeName=n5 Gres=gpu:2080:10 CPUs=32 RealMemory=64000 Sockets=2 CoresPerSocket=16 ThreadsPerCore=1 State=UNKNOWN
NodeName=m1 Gres=gpu:8000:2 CPUs=28 RealMemory=512000 Sockets=1 CoresPerSocket=28 ThreadsPerCore=1 State=UNKNOWN

PartitionName=crab Nodes=crab Default=NO MaxTime=INFINITE State=UP
#PartitionName=no_n1 Nodes=n[2-5] Default=NO MaxTime=INFINITE State=UP
PartitionName=all Nodes=crab,n[1-5],m1 Default=YES MaxTime=INFINITE State=UP
```

また、Generic ResoucesとしてGPUを管理下に入れることができる。
GPUを搭載するCompute nodeでは、GPU を自動認識してもらうために同じ場所に `gres.conf` を作成。
```bash
sudo echo "AutoDetect=nvml" >/usr/local/etc/gres.conf
```

firewallを適用している場合にはslurm通信用のポートを解放する。
```bash
sudo firewall-cmd --zone=external --add-port=6817/tcp --permanent
sudo firewall-cmd --zone=external --add-port=6817/udp --permanent
sudo firewall-cmd --zone=external --add-port=6818/tcp --permanent
sudo firewall-cmd --zone=external --add-port=6818/udp --permanent
sudo firewall-cmd --reload
```

`slurm.conf` の設定が上手く行っているかどうかを
`sudo slurmd -D` として対話的にデーモンを実行しながら確認。

エラーがなくなって、正常に起動するようになったら
Controller node で `slurmctld` デーモンを動かす
compute node で `slurmd` デーモンを動かす

`sinfo` で status が `drain` となってしまう場合は、
`scontrol show node ホスト名` などで理由をみる。

動作するようになったらsystemdへ登録する

```bash
sudo cp etc/slurmctld.service /etc/systemd/system
sudo cp etc/slurmd.service /etc/systemd/system

# On Controller node
sudo systemctl start slurmctld
sudo systemctl status slurmctld
sudo systemctl enable slurmctld

# On Compute node
sudo systemctl start slurmd
sudo systemctl status slurmd
sudo systemctl enable slurmd
```

---

### NIS

---

### NFS

---


